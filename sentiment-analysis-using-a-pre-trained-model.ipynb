{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-04T16:52:21.439965Z","iopub.execute_input":"2023-04-04T16:52:21.441046Z","iopub.status.idle":"2023-04-04T16:52:21.462509Z","shell.execute_reply.started":"2023-04-04T16:52:21.440989Z","shell.execute_reply":"2023-04-04T16:52:21.461229Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Importing the libraries ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom transformers import DistilBertTokenizerFast\nfrom transformers import TFDistilBertForSequenceClassification\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:52:25.914159Z","iopub.execute_input":"2023-04-04T16:52:25.914887Z","iopub.status.idle":"2023-04-04T16:52:25.920587Z","shell.execute_reply.started":"2023-04-04T16:52:25.914848Z","shell.execute_reply":"2023-04-04T16:52:25.919253Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Loading the dataset","metadata":{}},{"cell_type":"code","source":"df= pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:54:08.286764Z","iopub.execute_input":"2023-04-04T16:54:08.287153Z","iopub.status.idle":"2023-04-04T16:54:09.604945Z","shell.execute_reply.started":"2023-04-04T16:54:08.287119Z","shell.execute_reply":"2023-04-04T16:54:09.603821Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:54:20.831209Z","iopub.execute_input":"2023-04-04T16:54:20.832015Z","iopub.status.idle":"2023-04-04T16:54:20.853052Z","shell.execute_reply.started":"2023-04-04T16:54:20.831975Z","shell.execute_reply":"2023-04-04T16:54:20.851680Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                                  review sentiment\n0      One of the other reviewers has mentioned that ...  positive\n1      A wonderful little production. <br /><br />The...  positive\n2      I thought this was a wonderful way to spend ti...  positive\n3      Basically there's a family where a little boy ...  negative\n4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n...                                                  ...       ...\n49995  I thought this movie did a down right good job...  positive\n49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n49997  I am a Catholic taught in parochial elementary...  negative\n49998  I'm going to have to disagree with the previou...  negative\n49999  No one expects the Star Trek movies to be high...  negative\n\n[50000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49995</th>\n      <td>I thought this movie did a down right good job...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>49996</th>\n      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>49997</th>\n      <td>I am a Catholic taught in parochial elementary...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>49998</th>\n      <td>I'm going to have to disagree with the previou...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>49999</th>\n      <td>No one expects the Star Trek movies to be high...</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n<p>50000 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#  Split the data into training, validation, and testing sets","metadata":{}},{"cell_type":"code","source":"train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\ntrain_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T17:09:55.049981Z","iopub.execute_input":"2023-04-04T17:09:55.050375Z","iopub.status.idle":"2023-04-04T17:09:55.070057Z","shell.execute_reply.started":"2023-04-04T17:09:55.050318Z","shell.execute_reply":"2023-04-04T17:09:55.069106Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Load the tokenizer and encode the data","metadata":{}},{"cell_type":"code","source":"tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\ntrain_texts = train_data['review'].tolist()\ntrain_labels = train_data['sentiment'].tolist()\ntrain_labels = [1 if label == \"positive\" else 0 for label in train_labels]\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True)\n\nval_texts = val_data['review'].tolist()\nval_labels = val_data['sentiment'].tolist()\nval_labels = [1 if label == \"positive\" else 0 for label in val_labels]\nval_encodings = tokenizer(val_texts, truncation=True, padding=True)\n\ntest_texts = test_data['review'].tolist()\ntest_labels = test_data['sentiment'].tolist()\ntest_labels = [1 if label == \"positive\" else 0 for label in test_labels]\ntest_encodings = tokenizer(test_texts, truncation=True, padding=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T17:11:35.220194Z","iopub.execute_input":"2023-04-04T17:11:35.221124Z","iopub.status.idle":"2023-04-04T17:12:19.534164Z","shell.execute_reply.started":"2023-04-04T17:11:35.221082Z","shell.execute_reply":"2023-04-04T17:12:19.533064Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb86ab9e82d3422ea8ed6b6b99dc0a40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"384a59cfd49749c3ab112203ba7bc689"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b4ec5ed22a04132b567c64b01a5acb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0749a398af49437cbf634a4e1a102fd1"}},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Convert the encoded inputs to TensorFlow datasets","metadata":{}},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels)).shuffle(10000).batch(16)\nval_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), val_labels)).batch(64)\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels)).batch(64)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T17:17:38.583622Z","iopub.execute_input":"2023-04-04T17:17:38.584268Z","iopub.status.idle":"2023-04-04T17:19:51.368375Z","shell.execute_reply.started":"2023-04-04T17:17:38.584227Z","shell.execute_reply":"2023-04-04T17:19:51.367140Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Load and fine-tune the model","metadata":{}},{"cell_type":"code","source":"model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\nmodel.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n\nhistory = model.fit(train_dataset, validation_data=val_dataset, epochs=3)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T17:50:31.868598Z","iopub.execute_input":"2023-04-04T17:50:31.869605Z","iopub.status.idle":"2023-04-04T18:47:25.243367Z","shell.execute_reply.started":"2023-04-04T17:50:31.869547Z","shell.execute_reply":"2023-04-04T18:47:25.242302Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_layer_norm', 'vocab_transform', 'vocab_projector']\n- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_39', 'classifier', 'pre_classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n2000/2000 [==============================] - 1158s 565ms/step - loss: 0.2481 - accuracy: 0.8990 - val_loss: 0.1937 - val_accuracy: 0.9218\nEpoch 2/3\n2000/2000 [==============================] - 1127s 563ms/step - loss: 0.1339 - accuracy: 0.9523 - val_loss: 0.2151 - val_accuracy: 0.9314\nEpoch 3/3\n2000/2000 [==============================] - 1127s 564ms/step - loss: 0.0729 - accuracy: 0.9754 - val_loss: 0.2147 - val_accuracy: 0.9240\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#  Evaluate the model on the testing set","metadata":{}},{"cell_type":"code","source":"results = model.evaluate(test_dataset)\nprint(\"test loss, test accuracy:\", results)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T18:49:43.144566Z","iopub.execute_input":"2023-04-04T18:49:43.145573Z","iopub.status.idle":"2023-04-04T18:51:23.038386Z","shell.execute_reply.started":"2023-04-04T18:49:43.145533Z","shell.execute_reply":"2023-04-04T18:51:23.037351Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"157/157 [==============================] - 100s 636ms/step - loss: 0.2129 - accuracy: 0.9261\ntest loss, test accuracy: [0.21293900907039642, 0.9261000156402588]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}}]}